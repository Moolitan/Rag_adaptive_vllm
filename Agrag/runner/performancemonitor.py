import time
from typing import Any, Dict, List
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult

class DataCollector(BaseCallbackHandler):
    def __init__(self):
        self.starts = {}
        # è¿™é‡Œå®šä¹‰ä¸€ä¸ªå®¹å™¨æ¥å­˜æ•°æ®
        self.records = [] 

    # --- ç›‘æ§ LLM å¼€å§‹ ---
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        run_id = kwargs.get("run_id")
        name = serialized.get("name") if serialized else None
        self.starts[run_id] = {
            "start_time": time.time(),
            "type": "llm",
            "name": name,
            "prompt_preview": prompts[0][:50] if prompts else ""
        }

    # --- ç›‘æ§ LLM ç»“æŸ ---
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        run_id = kwargs.get("run_id")
        start_data = self.starts.pop(run_id, {})
        end_time = time.time()
        
        # æå–ç”Ÿæˆå†…å®¹
        generation = response.generations[0][0].text
        # å°è¯•æå– token usage (å–å†³äº vLLM æ˜¯å¦è¿”å›)
        usage = response.llm_output.get("token_usage", {}) if response.llm_output else {}

        # æŠŠæ•´ç†å¥½çš„æ•°æ®å­˜è¿› self.records
        record = {
            "event": "llm_call",
            "model": start_data.get("name"),
            "latency": end_time - start_data.get("start_time", end_time),
            "timestamp": end_time,
            "prompt": start_data.get("prompt_preview"),
            "response": generation, # è¿™é‡Œæ‹¿åˆ°äº† LLM çš„å›ç­”
            "input_tokens": usage.get("prompt_tokens", 0),
            "output_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0)
        }
        self.records.append(record)

    # --- ç›‘æ§ Tool/Node å¼€å§‹ ---
    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
        run_id = kwargs.get("run_id")
        # serialized å¯èƒ½ä¸º None
        if serialized is None:
            return
        name = serialized.get("name")
        # è¿‡æ»¤æ‰ LangGraph å†…éƒ¨çš„ä¸€äº›æ‚é¡¹ chainï¼Œåªå…³æ³¨ä¸»è¦çš„ Node
        if name and "LangGraph" not in name:
            self.starts[run_id] = {
                "start_time": time.time(),
                "type": "node",
                "name": name
            }

    # --- ç›‘æ§ Tool/Node ç»“æŸ ---
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        run_id = kwargs.get("run_id")
        start_data = self.starts.pop(run_id, None)
        if start_data: # åªæœ‰åœ¨ start é‡Œè®°å½•è¿‡çš„æ‰å¤„ç†
            end_time = time.time()
            record = {
                "event": "node_execution",
                "node_name": start_data["name"],
                "latency": end_time - start_data["start_time"],
                "timestamp": end_time,
                # "outputs": outputs # å¦‚æœéœ€è¦ä¸­é—´æ•°æ®ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Š
            }
            self.records.append(record)

# ==============================================================================
import requests
import time

VLLM_METRICS_URL = "http://localhost:8000/metrics"

def check_vllm_status():
    try:
        response = requests.get(VLLM_METRICS_URL)
        data = response.text
        
        # è§£æä½ éœ€è¦çš„æ•°æ®ï¼ŒvLLM è¿”å›çš„æ˜¯ Prometheus æ ¼å¼
        metrics = {}
        for line in data.split('\n'):
            if line.startswith("#") or not line: continue
            
            # ç¤ºä¾‹ï¼šæŠ“å–æ­£åœ¨è¿è¡Œçš„è¯·æ±‚æ•°
            if "vllm:num_requests_running" in line:
                metrics["running_reqs"] = float(line.split()[-1])
            
            # ç¤ºä¾‹ï¼šæŠ“å– KV Cache ä½¿ç”¨ç‡ (æ˜¾å­˜ç›¸å…³)
            if "vllm:gpu_cache_usage_perc" in line:
                metrics["gpu_cache"] = float(line.split()[-1])
                
            # ç¤ºä¾‹ï¼šæŠ“å– Token ç”Ÿæˆé€Ÿåº¦
            if "vllm:avg_generation_throughput_toks_per_s" in line:
                metrics["gen_speed"] = float(line.split()[-1])

        print(f"ğŸ“Š vLLM Status: Running={metrics.get('running_reqs', 0)} | "
              f"GPU Cache={metrics.get('gpu_cache', 0)*100:.1f}% | "
              f"Speed={metrics.get('gen_speed', 0):.1f} tok/s")

    except Exception as e:
        print(f"æ— æ³•è¿æ¥ vLLM: {e}")

# ä½ å¯ä»¥åœ¨ LangGraph è·‘ä»»åŠ¡çš„æ—¶å€™ï¼Œå•ç‹¬å¾ªç¯è°ƒç”¨è¿™ä¸ªå‡½æ•°
check_vllm_status()