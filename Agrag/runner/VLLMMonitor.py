import requests
import time
import threading
import csv
import os
from datetime import datetime
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path


class Analyzer:
    """æ¯ä¸ª analyzer è¿”å›ž (notes, derived_fields)."""
    name: str = "base"

    def analyze(self, current: Dict[str, float], deltas: Dict[str, float]) -> (List[str], Dict[str, Any]):
        return [], {}


class TokenRateAnalyzer(Analyzer):
    """
    å…³æ³¨ token å¢žé‡ & token/sã€‚
    æ³¨æ„:token_total çš„ delta æœ¬èº«ä¸æ˜¯â€œå‘½ä¸­/ä¸å‘½ä¸­â€,åªæ˜¯ workload çš„å˜åŒ–ã€‚
    """
    name = "token_rate"

    def __init__(self, interval_s: float):
        self.interval_s = max(interval_s, 1e-6)

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        dp = float(deltas.get("prompt", 0.0))
        dg = float(deltas.get("gen", 0.0))

        derived["prompt_toks_delta"] = dp
        derived["gen_toks_delta"] = dg
        derived["prompt_toks_per_s"] = dp / self.interval_s
        derived["gen_toks_per_s"] = dg / self.interval_s

        if dg > 0 and dp == 0:
            notes.append("ðŸ§  gen_only")
        if dp > 0 and dg == 0:
            notes.append("ðŸ“¥ prefill_only")

        return notes, derived


class QueueAnalyzer(Analyzer):
    """å…³æ³¨ waiting / running."""
    name = "queue"

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        waiting = int(current.get("waiting", 0))
        running = int(current.get("running", 0))

        derived["waiting"] = waiting
        derived["running"] = running

        if waiting > 0 and running > 0:
            notes.append("âš ï¸ queueing")
        elif waiting > 0 and running == 0:
            notes.append("â³ backlog_no_runner")

        return notes, derived


class GpuCacheAnalyzer(Analyzer):
    """
    å…³æ³¨ gpu_cache_usage_perc(é€šå¸¸ 0~1)ã€‚
    """
    name = "gpu_cache"

    def __init__(self, high: float = 0.90, warn: float = 0.80):
        self.warn = warn
        self.high = high

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        usage = float(current.get("gpu_cache", 0.0))
        derived["gpu_cache_usage"] = usage

        if usage >= self.high:
            notes.append("ðŸ”¥ kv_cache_high")
        elif usage >= self.warn:
            notes.append("ðŸŸ  kv_cache_warn")

        return notes, derived


class CacheHintAnalyzer(Analyzer):
    """
    åªç»™å‡º "å¯èƒ½å‘½ä¸­/å¯èƒ½é‡ç®—" çš„ hint,
    å¹¶æŠŠé˜ˆå€¼åšæˆå¯é…ç½® + è¾“å‡ºåˆ° CSV ä¾¿äºŽåŽéªŒæ ¡å‡†ã€‚
    """
    name = "cache_hint"

    def __init__(self, hit_threshold: float = 50.0, recompute_threshold: float = 500.0):
        self.hit_threshold = hit_threshold
        self.recompute_threshold = recompute_threshold

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        dp = float(deltas.get("prompt", 0.0))

        # è¾“å‡ºé˜ˆå€¼,ä¾¿äºŽ CSV åŽå¤„ç†/å›žå½’æ ¡å‡†
        derived["cache_hit_threshold"] = self.hit_threshold
        derived["cache_recompute_threshold"] = self.recompute_threshold

        # åªåœ¨ç¡®å®žå‘ç”Ÿäº† prefill æ—¶æ‰è°ˆ"hint"
        if dp > 0:
            if dp <= self.hit_threshold:
                notes.append("âœ… cache_hint_hit")
                derived["cache_hint"] = "hit"
            elif dp >= self.recompute_threshold:
                notes.append("ðŸ§Š cache_hint_recompute")
                derived["cache_hint"] = "recompute"
            else:
                derived["cache_hint"] = "unknown"
        else:
            derived["cache_hint"] = "n/a"

        return notes, derived


class PrefixCacheAnalyzer(Analyzer):

    name = "prefix_cache"

    def analyze(self, current, deltas):

        notes = []
        derived = {}

        # åªè¦æœ‰æ–°çš„è¯·æ±‚è¿›å…¥prefillé˜¶æ®µï¼Œå°±æŠŠæ–°è¯·æ±‚çš„æŸ¥è¯¢ç´¯åŠ 
        # å¢žåŠ çš„æ•°é‡ = prompt çš„ token æ•°ï¼Œå°è¯•æŸ¥è¯¢å°±ç®— query
        # prefix_cache_queries = æ‰€æœ‰è¯·æ±‚çš„ prompt token æ€»æ•°
        queries_total = float(current.get("prefix_cache_queries", 0.0))

        # æˆåŠŸä»Ž prefix cache ä¸­å¤ç”¨çš„ token æ•°
        # ä¹Ÿæ˜¯ä¼šä¸æ–­ç´¯åŠ çš„
        hits_total = float(current.get("prefix_cache_hits", 0.0))

        # å¢žé‡ï¼ˆæœ¬æ¬¡é‡‡æ ·å‘¨æœŸå†…çš„å˜åŒ–é‡ï¼‰
        # ä¾‹å¦‚ï¼šä¸Šæ¬¡é‡‡æ · queries=1000ï¼Œæœ¬æ¬¡ queries=1500ï¼Œåˆ™ delta_queries=500
        delta_queries = float(deltas.get("prefix_cache_queries", 0.0))
        delta_hits = float(deltas.get("prefix_cache_hits", 0.0))


        # å‘½ä¸­æ•°ä¸èƒ½å¤§äºŽæŸ¥è¯¢æ•°ï¼ˆè¿™åœ¨é€»è¾‘ä¸Šæ˜¯ä¸å¯èƒ½çš„ï¼‰
        # å¦‚æžœå‡ºçŽ°è¿™ç§æƒ…å†µï¼Œè¯´æ˜Ž vLLM çš„ metrics æ•°æ®å¼‚å¸¸
        if hits_total > queries_total and queries_total > 0:
            # ä¿®æ­£ï¼šå°† hits é™åˆ¶ä¸ºä¸è¶…è¿‡ queries
            hits_total = queries_total

        if delta_hits > delta_queries and delta_queries > 0:
            # ä¿®æ­£ï¼šå°†å¢žé‡ hits é™åˆ¶ä¸ºä¸è¶…è¿‡å¢žé‡ queries
            delta_hits = delta_queries

        # ç´¯ç§¯å‘½ä¸­çŽ‡ = æ€»å‘½ä¸­æ•° / æ€»æŸ¥è¯¢æ•°
        # åæ˜ ä»Žå¯åŠ¨åˆ°çŽ°åœ¨çš„æ•´ä½“ç¼“å­˜æ•ˆæžœ
        if queries_total > 0:
            hitrate_cumulative = (hits_total / queries_total) * 100.0
        else:
            # åˆå§‹åŒ–é˜¶æ®µæˆ–ç©ºé—²æœŸï¼Œè¿˜æ²¡æœ‰ä»»ä½•æŸ¥è¯¢
            hitrate_cumulative = 0.0

        # å¢žé‡å‘½ä¸­çŽ‡ = æœ¬æ¬¡å‘½ä¸­æ•° / æœ¬æ¬¡æŸ¥è¯¢æ•°
        # åæ˜ å½“å‰æ—¶åˆ»çš„å®žæ—¶ç¼“å­˜æ•ˆæžœï¼Œæ¯”ç´¯ç§¯å‘½ä¸­çŽ‡æ›´æ•æ„Ÿ
        if delta_queries > 0:
            hitrate_delta = (delta_hits / delta_queries) * 100.0
        else:
            # æœ¬æ¬¡é‡‡æ ·å‘¨æœŸå†…æ²¡æœ‰æ–°çš„æŸ¥è¯¢
            # å¯èƒ½åŽŸå› ï¼š
            # 1. ç³»ç»Ÿç©ºé—²
            # 2. åªæœ‰ decode é˜¶æ®µï¼ˆç”Ÿæˆ tokenï¼‰ï¼Œæ²¡æœ‰ prefill é˜¶æ®µ
            hitrate_delta = 0.0

        derived["prefix_cache_queries_total"] = queries_total
        derived["prefix_cache_hits_total"] = hits_total
        derived["prefix_cache_hitrate_cumulative"] = hitrate_cumulative
        derived["prefix_cache_delta_queries"] = delta_queries
        derived["prefix_cache_delta_hits"] = delta_hits
        derived["prefix_cache_hitrate_delta"] = hitrate_delta


        return notes, derived


# ----------------------------
# ä¸»ç›‘æŽ§ç±»
# ----------------------------

@dataclass
class MetricSpec:
    prom_name: str
    reduce: str = "sum"  # "sum" | "max" | "last"


class VLLMMonitor:
    def __init__(
        self,
        url: str = "http://localhost:8000/metrics",
        interval: float = 0.5,
        csv_path: str = "vllm_benchmark.csv",
        flush_every: int = 1,
    ):
        self.url = url
        self.interval = interval
        self.running = False

        # çŠ¶æ€è¿½è¸ª
        self.last_prompt_tokens: Optional[float] = None
        self.last_gen_tokens: Optional[float] = None
        self.last_prefix_cache_queries: Optional[float] = None
        self.last_prefix_cache_hits: Optional[float] = None

        # Prometheus æŒ‡æ ‡å®šä¹‰(ä¸åŒ vLLM ç‰ˆæœ¬å¯èƒ½åå­—ä¸åŒ,æ”¹è¿™é‡Œ)
        self.METRICS: Dict[str, MetricSpec] = {
            "prompt_total": MetricSpec("vllm:prompt_tokens_total", reduce="sum"),
            "gen_total": MetricSpec("vllm:generation_tokens_total", reduce="sum"),
            "running": MetricSpec("vllm:num_requests_running", reduce="last"),
            "waiting": MetricSpec("vllm:num_requests_waiting", reduce="last"),
            "gpu_cache": MetricSpec("vllm:kv_cache_usage_perc", reduce="last"),
            "prefix_cache_queries": MetricSpec("vllm:prefix_cache_queries_total", reduce="sum"),
            "prefix_cache_hits": MetricSpec("vllm:prefix_cache_hits_total", reduce="sum"),
        }

        # analyzers:å¯ä»¥è‡ªç”±å¢žåˆ /è°ƒæ•´é¡ºåº
        self.analyzers: List[Analyzer] = [
            TokenRateAnalyzer(interval_s=self.interval),
            # QueueAnalyzer(),
            # GpuCacheAnalyzer(warn=0.80, high=0.90),
            # CacheHintAnalyzer(hit_threshold=50.0, recompute_threshold=500.0),
            PrefixCacheAnalyzer(),
        ]

        # CSV å®žæ—¶å†™å…¥
        self.csv_path = csv_path
        self.flush_every = max(int(flush_every), 1)
        self._csv_f = None
        self._csv_writer = None
        self._csv_header_written = False
        self._rows_since_flush = 0

        # è®°å½•å­—æ®µæ¨¡æ¿(ç¬¬ä¸€æ¬¡è¿è¡ŒåŽåŠ¨æ€æ‰©å±•)
        self._base_fields = ["time"]
        self._last_fieldnames: Optional[List[str]] = None

    # ---------- lifecycle ----------

    def start(self):
        if self.running:
            return
        self.running = True
        self._open_csv()
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()
        print(f"ðŸš€ vLLM ç›‘æŽ§å·²å¯åŠ¨ (freq={self.interval}s, csv={self.csv_path})")

    def stop(self):
        self.running = False
        if hasattr(self, "thread"):
            self.thread.join(timeout=2)
        self._close_csv()
        print("ðŸ›‘ vLLM ç›‘æŽ§å·²åœæ­¢")

    # ---------- core loop ----------

    def _loop(self):
        while self.running:
            try:
                self._fetch_process_record()
            except requests.exceptions.ConnectionError:
                print(f"âš ï¸ æ— æ³•è¿žæŽ¥ vLLM ({self.url}),è¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨...")
            except Exception as e:
                print(f"Monitor Error: {e}")
            time.sleep(self.interval)

    def _fetch_process_record(self):
        raw = self._get_raw_data()
        if not raw:
            return

        current = self._parse_prometheus(raw)
        deltas = self._compute_deltas(current)
        if deltas is None:
            # åˆå§‹åŒ–é˜¶æ®µ:ä¸è¾“å‡º/ä¸å†™ CSV(é¿å…ç¬¬ä¸€è¡Œ delta å·¨å¤§/æ— æ„ä¹‰)
            return

        notes, derived = self._run_analyzers(current, deltas)
        self._record_entry(current, deltas, notes, derived)

    # ---------- IO & parsing ----------

    def _get_raw_data(self) -> Optional[str]:
        resp = requests.get(self.url, timeout=2)
        if resp.status_code != 200:
            return None
        return resp.text

    def _parse_prometheus(self, text: str) -> Dict[str, float]:
        """
        Prometheus exposition æ ¼å¼:
          metric_name{label="x"} 123
          metric_name 456
        åŒåå¤šè¡Œæ—¶æŒ‰ reduce èšåˆã€‚
        """
        # æ”¶é›†æ¯ä¸ª prom_name çš„æ‰€æœ‰ value
        bucket: Dict[str, List[float]] = {spec.prom_name: [] for spec in self.METRICS.values()}

        for line in text.splitlines():
            if not line or line.startswith("#"):
                continue
            # ä»¥ç©ºæ ¼åˆ†éš”:å·¦è¾¹æ˜¯ name{labels} æˆ– name,å³è¾¹æ˜¯ value
            try:
                left, value_str = line.rsplit(" ", 1)
                value = float(value_str)
            except Exception:
                continue

            # åŽ»æŽ‰ labels,åªä¿ç•™ name
            name = left.split("{", 1)[0].strip()

            # åªæ”¶é›†æˆ‘ä»¬å…³å¿ƒçš„
            for spec in self.METRICS.values():
                if name == spec.prom_name:
                    bucket[spec.prom_name].append(value)

        parsed: Dict[str, float] = {}
        for key, spec in self.METRICS.items():
            values = bucket.get(spec.prom_name, [])
            if not values:
                parsed[key] = 0.0
                continue
            if spec.reduce == "max":
                parsed[key] = max(values)
            elif spec.reduce == "last":
                parsed[key] = values[-1]
            else:  # sum
                parsed[key] = sum(values)

        return parsed

    # ---------- state/delta ----------

    def _compute_deltas(self, current: Dict[str, float]) -> Optional[Dict[str, float]]:
        pt = float(current.get("prompt_total", 0.0))
        gt = float(current.get("gen_total", 0.0))
        pcq = float(current.get("prefix_cache_queries", 0.0))
        pch = float(current.get("prefix_cache_hits", 0.0))

        if self.last_prompt_tokens is None:
            self.last_prompt_tokens = pt
            self.last_gen_tokens = gt
            self.last_prefix_cache_queries = pcq
            self.last_prefix_cache_hits = pch
            return None

        deltas = {
            "prompt": pt - float(self.last_prompt_tokens),
            "gen": gt - float(self.last_gen_tokens),
            "prefix_cache_queries": pcq - float(self.last_prefix_cache_queries),
            "prefix_cache_hits": pch - float(self.last_prefix_cache_hits),
        }

        self.last_prompt_tokens = pt
        self.last_gen_tokens = gt
        self.last_prefix_cache_queries = pcq
        self.last_prefix_cache_hits = pch
        return deltas

    # ---------- analysis ----------

    def _run_analyzers(self, current, deltas) -> (List[str], Dict[str, Any]):
        notes_all: List[str] = []
        derived_all: Dict[str, Any] = {}

        for a in self.analyzers:
            notes, derived = a.analyze(current, deltas)
            if notes:
                notes_all.extend(notes)
            if derived:
                # analyzer å­—æ®µåå†²çªæ—¶,åŽè€…è¦†ç›–å‰è€…(ä¹Ÿå¯ä»¥æ”¹æˆæŠ¥é”™)
                derived_all.update(derived)

        return notes_all, derived_all

    # ---------- record & csv ----------

    def _record_entry(self, current, deltas, notes: List[str], derived: Dict[str, Any]):
        now = datetime.now().strftime("%H:%M:%S")

        # 1) ç»Ÿä¸€æ‰“å°(å°½é‡ç¨³å®šã€ç®€æ´)
        gpu_pct = float(current.get("gpu_cache", 0.0)) * 100.0
        log = (
            f"[{now}] "
            f"Î”P:{int(deltas['prompt'])} | Î”G:{int(deltas['gen'])} | "
            f"Run:{int(current.get('running', 0))} Wait:{int(current.get('waiting', 0))} | "
            f"KV:{gpu_pct:.1f}%"
        )
        if notes:
            log += " | " + " ".join(notes)
        print(log)

        # 2) è¡Œæ•°æ®(CSV:å»ºè®®â€œåŽŸå§‹ + delta + derived + notesâ€)
        row: Dict[str, Any] = {}
        row["time"] = now

        # åŽŸå§‹å€¼
        row.update({
            "prompt_total": float(current.get("prompt_total", 0.0)),
            "gen_total": float(current.get("gen_total", 0.0)),
            "running_raw": float(current.get("running", 0.0)),
            "waiting_raw": float(current.get("waiting", 0.0)),
            "gpu_cache_raw": float(current.get("gpu_cache", 0.0)),
        })

        # delta
        row.update({
            "delta_prompt": float(deltas.get("prompt", 0.0)),
            "delta_gen": float(deltas.get("gen", 0.0)),
        })

        # derived
        row.update(derived or {})

        # notes ä¸²
        row["notes"] = " ".join(notes) if notes else ""

        self._append_csv_row(row)

    def _open_csv(self):
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        csv_dir = os.path.dirname(os.path.abspath(self.csv_path))
        os.makedirs(csv_dir, exist_ok=True)

        # ä½¿ç”¨ "w" æ¨¡å¼è¦†ç›–å†™ï¼Œè€Œä¸æ˜¯ "a" è¿½åŠ å†™
        self._csv_f = open(self.csv_path, "w", newline="", encoding="utf-8")
        # writer éœ€è¦ fieldnames,é¦–æ¬¡å†™å…¥æ—¶å†ç¡®å®š
        self._csv_writer = None
        self._csv_header_written = False  # æ–°æ–‡ä»¶ï¼Œè¡¨å¤´æœªå†™å…¥
        self._rows_since_flush = 0

    def _close_csv(self):
        if self._csv_f:
            try:
                self._csv_f.flush()
            except Exception:
                pass
            try:
                self._csv_f.close()
            except Exception:
                pass
        self._csv_f = None
        self._csv_writer = None

    def _append_csv_row(self, row: Dict[str, Any]):
        """
        å…³é”®ç‚¹:å­—æ®µåå¯èƒ½éšç€ analyzer å¢žåŠ è€Œæ‰©å±•ã€‚
        è¿™é‡Œåšä¸€ä¸ªâ€œåŠ¨æ€è¡¨å¤´â€ç­–ç•¥:
        - å¦‚æžœå½“å‰ row æœ‰æ–°å­—æ®µ -> é‡æ–°å†™ä¸€ä¸ªæ–°æ–‡ä»¶æœ€æ ‡å‡†
          ä½†å®žçŽ°å¤æ‚ã€‚
        - è¿™é‡Œé‡‡å–â€œä¿å®ˆç­–ç•¥â€:fieldnames = åŽ†å²å­—æ®µ âˆª å½“å‰å­—æ®µ(æŽ’åºåŽç¨³å®š)
          è‹¥å‘çŽ°æ–°å­—æ®µ,ä¼šåœ¨åŽç»­è¡ŒåŒ…å«è¿™äº›å­—æ®µ(æ—§è¡Œè‡ªç„¶ä¸ºç©º)ã€‚
        """
        if not self._csv_f:
            return

        # è®¡ç®— fieldnames(ç¨³å®šæŽ’åº:time æ”¾æœ€å‰,å…¶ä½™æŒ‰å­—æ¯)
        keys = list(row.keys())
        if self._last_fieldnames is None:
            fieldnames = ["time"] + sorted([k for k in keys if k != "time"])
        else:
            merged = set(self._last_fieldnames) | set(keys)
            merged.discard("time")
            fieldnames = ["time"] + sorted(list(merged))

        # è‹¥ fieldnames å˜åŒ–,éœ€è¦é‡æ–°æž„å»º writer
        if self._csv_writer is None or fieldnames != self._last_fieldnames:
            self._csv_writer = csv.DictWriter(self._csv_f, fieldnames=fieldnames)
            self._last_fieldnames = fieldnames
            if not self._csv_header_written:
                self._csv_writer.writeheader()
                self._csv_header_written = True
            else:
                # å·²æœ‰æ—§æ–‡ä»¶ä½†è¡¨å¤´å¯èƒ½ä¸åŒ:è¿™é‡Œä¸é‡å†™è¡¨å¤´,é¿å…ç ´åæ–‡ä»¶ç»“æž„
                #(å¦‚æžœå¸Œæœ›ä¸¥æ ¼ä¸€è‡´,å»ºè®®â€œæ–°å­—æ®µå‡ºçŽ°å°±æ–°èµ·ä¸€ä¸ªæ–‡ä»¶â€)
                pass

        self._csv_writer.writerow(row)
        self._rows_since_flush += 1

        if self._rows_since_flush >= self.flush_every:
            try:
                self._csv_f.flush()
            except Exception:
                pass
            self._rows_since_flush = 0


if __name__ == "__main__":
    mon = VLLMMonitor(
        url="http://localhost:8000/metrics",
        interval=0.5,
        csv_path="vllm_benchmark.csv",
        flush_every=1,
    )
    mon.start()
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        mon.stop()


