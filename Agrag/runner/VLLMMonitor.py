import requests
import time
import threading
import csv
import os
from datetime import datetime
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable


class Analyzer:
    """æ¯ä¸ª analyzer è¿”å›ž (notes, derived_fields)."""
    name: str = "base"

    def analyze(self, current: Dict[str, float], deltas: Dict[str, float]) -> (List[str], Dict[str, Any]):
        return [], {}


class TokenRateAnalyzer(Analyzer):
    """
    å…³æ³¨ token å¢žé‡ & token/sã€‚
    æ³¨æ„:token_total çš„ delta æœ¬èº«ä¸æ˜¯â€œå‘½ä¸­/ä¸å‘½ä¸­â€,åªæ˜¯ workload çš„å˜åŒ–ã€‚
    """
    name = "token_rate"

    def __init__(self, interval_s: float):
        self.interval_s = max(interval_s, 1e-6)

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        dp = float(deltas.get("prompt", 0.0))
        dg = float(deltas.get("gen", 0.0))

        derived["prompt_toks_delta"] = dp
        derived["gen_toks_delta"] = dg
        derived["prompt_toks_per_s"] = dp / self.interval_s
        derived["gen_toks_per_s"] = dg / self.interval_s

        if dg > 0 and dp == 0:
            notes.append("ðŸ§  gen_only")
        if dp > 0 and dg == 0:
            notes.append("ðŸ“¥ prefill_only")

        return notes, derived


class QueueAnalyzer(Analyzer):
    """å…³æ³¨ waiting / running."""
    name = "queue"

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        waiting = int(current.get("waiting", 0))
        running = int(current.get("running", 0))

        derived["waiting"] = waiting
        derived["running"] = running

        if waiting > 0 and running > 0:
            notes.append("âš ï¸ queueing")
        elif waiting > 0 and running == 0:
            notes.append("â³ backlog_no_runner")

        return notes, derived


class GpuCacheAnalyzer(Analyzer):
    """
    å…³æ³¨ gpu_cache_usage_perc(é€šå¸¸ 0~1)ã€‚
    """
    name = "gpu_cache"

    def __init__(self, high: float = 0.90, warn: float = 0.80):
        self.warn = warn
        self.high = high

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        usage = float(current.get("gpu_cache", 0.0))
        derived["gpu_cache_usage"] = usage

        if usage >= self.high:
            notes.append("ðŸ”¥ kv_cache_high")
        elif usage >= self.warn:
            notes.append("ðŸŸ  kv_cache_warn")

        return notes, derived


class CacheHintAnalyzer(Analyzer):
    """
    åªç»™å‡º â€œå¯èƒ½å‘½ä¸­/å¯èƒ½é‡ç®—â€ çš„ hint,
    å¹¶æŠŠé˜ˆå€¼åšæˆå¯é…ç½® + è¾“å‡ºåˆ° CSV ä¾¿äºŽåŽéªŒæ ¡å‡†ã€‚
    """
    name = "cache_hint"

    def __init__(self, hit_threshold: float = 50.0, recompute_threshold: float = 500.0):
        self.hit_threshold = hit_threshold
        self.recompute_threshold = recompute_threshold

    def analyze(self, current, deltas):
        notes = []
        derived = {}

        dp = float(deltas.get("prompt", 0.0))

        # è¾“å‡ºé˜ˆå€¼,ä¾¿äºŽ CSV åŽå¤„ç†/å›žå½’æ ¡å‡†
        derived["cache_hit_threshold"] = self.hit_threshold
        derived["cache_recompute_threshold"] = self.recompute_threshold

        # åªåœ¨ç¡®å®žå‘ç”Ÿäº† prefill æ—¶æ‰è°ˆâ€œhintâ€
        if dp > 0:
            if dp <= self.hit_threshold:
                notes.append("âœ… cache_hint_hit")
                derived["cache_hint"] = "hit"
            elif dp >= self.recompute_threshold:
                notes.append("ðŸ§Š cache_hint_recompute")
                derived["cache_hint"] = "recompute"
            else:
                derived["cache_hint"] = "unknown"
        else:
            derived["cache_hint"] = "n/a"

        return notes, derived


# ----------------------------
# ä¸»ç›‘æŽ§ç±»
# ----------------------------

@dataclass
class MetricSpec:
    prom_name: str
    reduce: str = "sum"  # "sum" | "max" | "last"


class VLLMMonitor:
    def __init__(
        self,
        url: str = "http://localhost:8000/metrics",
        interval: float = 0.5,
        csv_path: str = "vllm_benchmark.csv",
        flush_every: int = 1,
    ):
        self.url = url
        self.interval = interval
        self.running = False

        # çŠ¶æ€è¿½è¸ª
        self.last_prompt_tokens: Optional[float] = None
        self.last_gen_tokens: Optional[float] = None

        # Prometheus æŒ‡æ ‡å®šä¹‰(ä¸åŒ vLLM ç‰ˆæœ¬å¯èƒ½åå­—ä¸åŒ,æ”¹è¿™é‡Œ)
        self.METRICS: Dict[str, MetricSpec] = {
            "prompt_total": MetricSpec("vllm:prompt_tokens_total", reduce="sum"),
            "gen_total": MetricSpec("vllm:generation_tokens_total", reduce="sum"),
            "running": MetricSpec("vllm:num_requests_running", reduce="last"),
            "waiting": MetricSpec("vllm:num_requests_waiting", reduce="last"),
            "gpu_cache": MetricSpec("vllm:kv_cache_usage_perc", reduce="last"),
        }

        # analyzers:å¯ä»¥è‡ªç”±å¢žåˆ /è°ƒæ•´é¡ºåº
        self.analyzers: List[Analyzer] = [
            TokenRateAnalyzer(interval_s=self.interval),
            QueueAnalyzer(),
            GpuCacheAnalyzer(warn=0.80, high=0.90),
            CacheHintAnalyzer(hit_threshold=50.0, recompute_threshold=500.0),
        ]

        # CSV å®žæ—¶å†™å…¥
        self.csv_path = csv_path
        self.flush_every = max(int(flush_every), 1)
        self._csv_f = None
        self._csv_writer = None
        self._csv_header_written = False
        self._rows_since_flush = 0

        # è®°å½•å­—æ®µæ¨¡æ¿(ç¬¬ä¸€æ¬¡è¿è¡ŒåŽåŠ¨æ€æ‰©å±•)
        self._base_fields = ["time"]
        self._last_fieldnames: Optional[List[str]] = None

    # ---------- lifecycle ----------

    def start(self):
        if self.running:
            return
        self.running = True
        self._open_csv()
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()
        print(f"ðŸš€ vLLM ç›‘æŽ§å·²å¯åŠ¨ (freq={self.interval}s, csv={self.csv_path})")

    def stop(self):
        self.running = False
        if hasattr(self, "thread"):
            self.thread.join(timeout=2)
        self._close_csv()
        print("ðŸ›‘ vLLM ç›‘æŽ§å·²åœæ­¢")

    # ---------- core loop ----------

    def _loop(self):
        while self.running:
            try:
                self._fetch_process_record()
            except requests.exceptions.ConnectionError:
                print(f"âš ï¸ æ— æ³•è¿žæŽ¥ vLLM ({self.url}),è¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨...")
            except Exception as e:
                print(f"Monitor Error: {e}")
            time.sleep(self.interval)

    def _fetch_process_record(self):
        raw = self._get_raw_data()
        if not raw:
            return

        current = self._parse_prometheus(raw)
        deltas = self._compute_deltas(current)
        if deltas is None:
            # åˆå§‹åŒ–é˜¶æ®µ:ä¸è¾“å‡º/ä¸å†™ CSV(é¿å…ç¬¬ä¸€è¡Œ delta å·¨å¤§/æ— æ„ä¹‰)
            return

        notes, derived = self._run_analyzers(current, deltas)
        self._record_entry(current, deltas, notes, derived)

    # ---------- IO & parsing ----------

    def _get_raw_data(self) -> Optional[str]:
        resp = requests.get(self.url, timeout=2)
        if resp.status_code != 200:
            return None
        return resp.text

    def _parse_prometheus(self, text: str) -> Dict[str, float]:
        """
        Prometheus exposition æ ¼å¼:
          metric_name{label="x"} 123
          metric_name 456
        åŒåå¤šè¡Œæ—¶æŒ‰ reduce èšåˆã€‚
        """
        # æ”¶é›†æ¯ä¸ª prom_name çš„æ‰€æœ‰ value
        bucket: Dict[str, List[float]] = {spec.prom_name: [] for spec in self.METRICS.values()}

        for line in text.splitlines():
            if not line or line.startswith("#"):
                continue
            # ä»¥ç©ºæ ¼åˆ†éš”:å·¦è¾¹æ˜¯ name{labels} æˆ– name,å³è¾¹æ˜¯ value
            try:
                left, value_str = line.rsplit(" ", 1)
                value = float(value_str)
            except Exception:
                continue

            # åŽ»æŽ‰ labels,åªä¿ç•™ name
            name = left.split("{", 1)[0].strip()

            # åªæ”¶é›†æˆ‘ä»¬å…³å¿ƒçš„
            for spec in self.METRICS.values():
                if name == spec.prom_name:
                    bucket[spec.prom_name].append(value)

        parsed: Dict[str, float] = {}
        for key, spec in self.METRICS.items():
            values = bucket.get(spec.prom_name, [])
            if not values:
                parsed[key] = 0.0
                continue
            if spec.reduce == "max":
                parsed[key] = max(values)
            elif spec.reduce == "last":
                parsed[key] = values[-1]
            else:  # sum
                parsed[key] = sum(values)

        return parsed

    # ---------- state/delta ----------

    def _compute_deltas(self, current: Dict[str, float]) -> Optional[Dict[str, float]]:
        pt = float(current.get("prompt_total", 0.0))
        gt = float(current.get("gen_total", 0.0))

        if self.last_prompt_tokens is None:
            self.last_prompt_tokens = pt
            self.last_gen_tokens = gt
            return None

        deltas = {
            "prompt": pt - float(self.last_prompt_tokens),
            "gen": gt - float(self.last_gen_tokens),
        }

        self.last_prompt_tokens = pt
        self.last_gen_tokens = gt
        return deltas

    # ---------- analysis ----------

    def _run_analyzers(self, current, deltas) -> (List[str], Dict[str, Any]):
        notes_all: List[str] = []
        derived_all: Dict[str, Any] = {}

        for a in self.analyzers:
            notes, derived = a.analyze(current, deltas)
            if notes:
                notes_all.extend(notes)
            if derived:
                # analyzer å­—æ®µåå†²çªæ—¶,åŽè€…è¦†ç›–å‰è€…(ä¹Ÿå¯ä»¥æ”¹æˆæŠ¥é”™)
                derived_all.update(derived)

        return notes_all, derived_all

    # ---------- record & csv ----------

    def _record_entry(self, current, deltas, notes: List[str], derived: Dict[str, Any]):
        now = datetime.now().strftime("%H:%M:%S")

        # 1) ç»Ÿä¸€æ‰“å°(å°½é‡ç¨³å®šã€ç®€æ´)
        gpu_pct = float(current.get("gpu_cache", 0.0)) * 100.0
        log = (
            f"[{now}] "
            f"Î”P:{int(deltas['prompt'])} | Î”G:{int(deltas['gen'])} | "
            f"Run:{int(current.get('running', 0))} Wait:{int(current.get('waiting', 0))} | "
            f"KV:{gpu_pct:.1f}%"
        )
        if notes:
            log += " | " + " ".join(notes)
        print(log)

        # 2) è¡Œæ•°æ®(CSV:å»ºè®®â€œåŽŸå§‹ + delta + derived + notesâ€)
        row: Dict[str, Any] = {}
        row["time"] = now

        # åŽŸå§‹å€¼
        row.update({
            "prompt_total": float(current.get("prompt_total", 0.0)),
            "gen_total": float(current.get("gen_total", 0.0)),
            "running_raw": float(current.get("running", 0.0)),
            "waiting_raw": float(current.get("waiting", 0.0)),
            "gpu_cache_raw": float(current.get("gpu_cache", 0.0)),
        })

        # delta
        row.update({
            "delta_prompt": float(deltas.get("prompt", 0.0)),
            "delta_gen": float(deltas.get("gen", 0.0)),
        })

        # derived
        row.update(derived or {})

        # notes ä¸²
        row["notes"] = " ".join(notes) if notes else ""

        self._append_csv_row(row)

    def _open_csv(self):
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        csv_dir = os.path.dirname(os.path.abspath(self.csv_path))
        os.makedirs(csv_dir, exist_ok=True)

        self._csv_f = open(self.csv_path, "a", newline="", encoding="utf-8")
        # writer éœ€è¦ fieldnames,é¦–æ¬¡å†™å…¥æ—¶å†ç¡®å®š
        self._csv_writer = None
        self._csv_header_written = os.path.getsize(self.csv_path) > 0
        self._rows_since_flush = 0

    def _close_csv(self):
        if self._csv_f:
            try:
                self._csv_f.flush()
            except Exception:
                pass
            try:
                self._csv_f.close()
            except Exception:
                pass
        self._csv_f = None
        self._csv_writer = None

    def _append_csv_row(self, row: Dict[str, Any]):
        """
        å…³é”®ç‚¹:å­—æ®µåå¯èƒ½éšç€ analyzer å¢žåŠ è€Œæ‰©å±•ã€‚
        è¿™é‡Œåšä¸€ä¸ªâ€œåŠ¨æ€è¡¨å¤´â€ç­–ç•¥:
        - å¦‚æžœå½“å‰ row æœ‰æ–°å­—æ®µ -> é‡æ–°å†™ä¸€ä¸ªæ–°æ–‡ä»¶æœ€æ ‡å‡†
          ä½†å®žçŽ°å¤æ‚ã€‚
        - è¿™é‡Œé‡‡å–â€œä¿å®ˆç­–ç•¥â€:fieldnames = åŽ†å²å­—æ®µ âˆª å½“å‰å­—æ®µ(æŽ’åºåŽç¨³å®š)
          è‹¥å‘çŽ°æ–°å­—æ®µ,ä¼šåœ¨åŽç»­è¡ŒåŒ…å«è¿™äº›å­—æ®µ(æ—§è¡Œè‡ªç„¶ä¸ºç©º)ã€‚
        """
        if not self._csv_f:
            return

        # è®¡ç®— fieldnames(ç¨³å®šæŽ’åº:time æ”¾æœ€å‰,å…¶ä½™æŒ‰å­—æ¯)
        keys = list(row.keys())
        if self._last_fieldnames is None:
            fieldnames = ["time"] + sorted([k for k in keys if k != "time"])
        else:
            merged = set(self._last_fieldnames) | set(keys)
            merged.discard("time")
            fieldnames = ["time"] + sorted(list(merged))

        # è‹¥ fieldnames å˜åŒ–,éœ€è¦é‡æ–°æž„å»º writer
        if self._csv_writer is None or fieldnames != self._last_fieldnames:
            self._csv_writer = csv.DictWriter(self._csv_f, fieldnames=fieldnames)
            self._last_fieldnames = fieldnames
            if not self._csv_header_written:
                self._csv_writer.writeheader()
                self._csv_header_written = True
            else:
                # å·²æœ‰æ—§æ–‡ä»¶ä½†è¡¨å¤´å¯èƒ½ä¸åŒ:è¿™é‡Œä¸é‡å†™è¡¨å¤´,é¿å…ç ´åæ–‡ä»¶ç»“æž„
                #(å¦‚æžœå¸Œæœ›ä¸¥æ ¼ä¸€è‡´,å»ºè®®â€œæ–°å­—æ®µå‡ºçŽ°å°±æ–°èµ·ä¸€ä¸ªæ–‡ä»¶â€)
                pass

        self._csv_writer.writerow(row)
        self._rows_since_flush += 1

        if self._rows_since_flush >= self.flush_every:
            try:
                self._csv_f.flush()
            except Exception:
                pass
            self._rows_since_flush = 0


if __name__ == "__main__":
    mon = VLLMMonitor(
        url="http://localhost:8000/metrics",
        interval=0.5,
        csv_path="vllm_benchmark.csv",
        flush_every=1,
    )
    mon.start()
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        mon.stop()
